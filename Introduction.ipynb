{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Prototype U.S. CMS analysis facility\n",
    "\n",
    "by \n",
    "\n",
    "Mat Adamec, Ken Bloom, **Oksana Shadura**, \n",
    "*University of Nebraska, Lincoln*\n",
    "\n",
    "Garhan Attebury, Carl Lundstedt, Derek Weitzel,\n",
    "*University of Nebraska Holland Computing Center*\n",
    "\n",
    "Mátyás Selmeci,\n",
    "*University of Wisconsin, Madison*\n",
    "\n",
    "Brian Bockelman,\n",
    "*Morgridge Institute*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coffea - Columnar Object Framework For Effective Analysis\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3333797.svg)](https://doi.org/10.5281/zenodo.3333797)\n",
    "[Coffea Team](https://github.com/CoffeaTeam) && [Coffea Framework](https://github.com/CoffeaTeam/coffea)\n",
    "\n",
    "* Leveraging large data and data analysis tools from Python to provide an array-based syntax for manipulating HEP event data\n",
    "* Stark contrast to well established event loop techniques\n",
    "* \"+\" Tremendous potential to fundamentally change the time-to-science in HEP\n",
    "* \"+\" **Scales well horizontally with available muliple executors for efficient and flexible computations**\n",
    "* \"-\" Cannot easily utilize current analysis facilities (T2s) as the analysis is not grid friendly, it's meant to be quasi-interactive\n",
    "\n",
    "\n",
    "<img src=\"https://coffeateam.github.io/coffea/_images/columnar.png\" width=\"400\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Facility design: Coffea Local Executors\n",
    "\n",
    "Two local executors: *iterative_executor* and *futures_executor*:\n",
    "\n",
    "* The *iterative* executor simply processes each chunk of an input dataset in turn, using the current python thread.\n",
    "\n",
    "* The *futures* executor employs python multiprocessing to spawn multiple python processes that process chunks in parallel on the machine. **Processes are used rather than threads to avoid performance limitations due to the CPython global interpreter lock (GIL))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Facility design: Coffea Distributed Executors\n",
    "\n",
    "\n",
    "Four types of distributed executors:\n",
    "\n",
    " * the **Parsl** distributed executor, accessed via *parsl_executor*, \n",
    "\n",
    " * the **Dask** distributed executor, accessed via *dask_executor*,\n",
    "\n",
    " * the **Apache Spark** distributed executor, accessed via *run_spark_job*,\n",
    "\n",
    " * and the **Work Queue** distributed executor, accessed via *work_queue_executor*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dask: scalable analytics in Python\n",
    "\n",
    "* Dask provides flexible library for parallel computing in Python\n",
    "* Think of Dask as run-time parallel + cluster plugin for Python\n",
    "* Easily installed via Conda as the module “distributed”\n",
    "* NOT really designed with multi-user environments in mind  out-of-the-box\n",
    "* Integrates with HPC clusters running a variety of scheduler including SLURM & HTCondor via “dask-jobqueue”\n",
    "\n",
    "<img src=\"https://docs.dask.org/en/latest/_images/dask-overview.svg\" width=\"600\" height=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>inproc://192.168.49.162/1333/1</li>\n",
       "  <li><b>Dashboard: </b><a href='/user/oksana.shadura@cern.ch/proxy/8787/status' target='_blank'>/user/oksana.shadura@cern.ch/proxy/8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>4.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'inproc://192.168.49.162/1333/1' processes=1 threads=8, memory=4.00 GB>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, progress\n",
    "client = Client(processes=False, threads_per_worker=8,\n",
    "                n_workers=1, memory_limit='4GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 800.00 MB </td> <td> 8.00 MB </td></tr>\n",
       "    <tr><th> Shape </th><td> (10000, 10000) </td> <td> (1000, 1000) </td></tr>\n",
       "    <tr><th> Count </th><td> 100 Tasks </td><td> 100 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"170\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"12\" x2=\"120\" y2=\"12\" />\n",
       "  <line x1=\"0\" y1=\"24\" x2=\"120\" y2=\"24\" />\n",
       "  <line x1=\"0\" y1=\"36\" x2=\"120\" y2=\"36\" />\n",
       "  <line x1=\"0\" y1=\"48\" x2=\"120\" y2=\"48\" />\n",
       "  <line x1=\"0\" y1=\"60\" x2=\"120\" y2=\"60\" />\n",
       "  <line x1=\"0\" y1=\"72\" x2=\"120\" y2=\"72\" />\n",
       "  <line x1=\"0\" y1=\"84\" x2=\"120\" y2=\"84\" />\n",
       "  <line x1=\"0\" y1=\"96\" x2=\"120\" y2=\"96\" />\n",
       "  <line x1=\"0\" y1=\"108\" x2=\"120\" y2=\"108\" />\n",
       "  <line x1=\"0\" y1=\"120\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"120\" />\n",
       "  <line x1=\"24\" y1=\"0\" x2=\"24\" y2=\"120\" />\n",
       "  <line x1=\"36\" y1=\"0\" x2=\"36\" y2=\"120\" />\n",
       "  <line x1=\"48\" y1=\"0\" x2=\"48\" y2=\"120\" />\n",
       "  <line x1=\"60\" y1=\"0\" x2=\"60\" y2=\"120\" />\n",
       "  <line x1=\"72\" y1=\"0\" x2=\"72\" y2=\"120\" />\n",
       "  <line x1=\"84\" y1=\"0\" x2=\"84\" y2=\"120\" />\n",
       "  <line x1=\"96\" y1=\"0\" x2=\"96\" y2=\"120\" />\n",
       "  <line x1=\"108\" y1=\"0\" x2=\"108\" y2=\"120\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 120.000000,0.000000 120.000000,120.000000 0.000000,120.000000\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"140.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >10000</text>\n",
       "  <text x=\"140.000000\" y=\"60.000000\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,140.000000,60.000000)\">10000</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<random_sample, shape=(10000, 10000), dtype=float64, chunksize=(1000, 1000), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.array as da\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we got is a 10000x10000 array of random numbers, represented as many numpy arrays of size 1000x1000 (or smaller if the array cannot be divided evenly). In this case there are 100 (10x10) numpy arrays of size 1000x1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr>\n",
       "<td>\n",
       "<table>\n",
       "  <thead>\n",
       "    <tr><td> </td><th> Array </th><th> Chunk </th></tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr><th> Bytes </th><td> 40.00 kB </td> <td> 4.00 kB </td></tr>\n",
       "    <tr><th> Shape </th><td> (5000,) </td> <td> (500,) </td></tr>\n",
       "    <tr><th> Count </th><td> 430 Tasks </td><td> 10 Chunks </td></tr>\n",
       "    <tr><th> Type </th><td> float64 </td><td> numpy.ndarray </td></tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</td>\n",
       "<td>\n",
       "<svg width=\"170\" height=\"75\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"120\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"0\" y1=\"25\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"0\" y1=\"0\" x2=\"0\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"12\" y1=\"0\" x2=\"12\" y2=\"25\" />\n",
       "  <line x1=\"24\" y1=\"0\" x2=\"24\" y2=\"25\" />\n",
       "  <line x1=\"36\" y1=\"0\" x2=\"36\" y2=\"25\" />\n",
       "  <line x1=\"48\" y1=\"0\" x2=\"48\" y2=\"25\" />\n",
       "  <line x1=\"60\" y1=\"0\" x2=\"60\" y2=\"25\" />\n",
       "  <line x1=\"72\" y1=\"0\" x2=\"72\" y2=\"25\" />\n",
       "  <line x1=\"84\" y1=\"0\" x2=\"84\" y2=\"25\" />\n",
       "  <line x1=\"96\" y1=\"0\" x2=\"96\" y2=\"25\" />\n",
       "  <line x1=\"108\" y1=\"0\" x2=\"108\" y2=\"25\" />\n",
       "  <line x1=\"120\" y1=\"0\" x2=\"120\" y2=\"25\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"0.000000,0.000000 120.000000,0.000000 120.000000,25.412617 0.000000,25.412617\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"60.000000\" y=\"45.412617\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >5000</text>\n",
       "  <text x=\"140.000000\" y=\"12.706308\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(0,140.000000,12.706308)\">1</text>\n",
       "</svg>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<mean_agg-aggregate, shape=(5000,), dtype=float64, chunksize=(500,), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x + x.T\n",
    "z = y[::2, 5000:].mean(axis=1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.96 s, sys: 486 ms, total: 3.45 s\n",
      "Wall time: 893 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.00056182, 1.0010036 , 0.99943392, ..., 0.99328108, 0.99941228,\n",
       "       1.00043039])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Dask?\n",
    "\n",
    "* <span style=\"color:green\"> Dask Scales out to Clusters </span>: Dask figures out how to break up large computations and route parts of them efficiently onto distributed hardware.\n",
    "\n",
    "* <span style=\"color:blue\"> Dask Scales Down to Single Computers </span>: Dask can enable efficient parallel computations on single machines by leveraging their multi-core CPUs and streaming data efficiently from disk.\n",
    "\n",
    "* <span style=\"color:orange\"> Dask Integrates Natively with Python Code </span>: Python includes computational libraries like Numpy, Pandas, and Scikit-Learn, and many others for data access, plotting, statistics, image and signal processing, and more.\n",
    "\n",
    "* <span style=\"color:red\"> Dask Supports Complex Applications </span>: Dask helps exposing low-level APIs to its internal task scheduler which is capable of executing very advanced computations. (e.g. the ability to build their own parallel computing system using the same engine that powers Dask’s arrays, DataFrames, and machine learning algorithms, but now with the institution’s own custom logic) **[similiar to our use case]**\n",
    "\n",
    "* <span style=\"color:purple\"> Dask Delivers Responsive Feedback </span>: monitoring via real-time and responsive dashboard, installed profiler, embedded IPython kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Requirements for Analysis Facility @ T2\n",
    "\n",
    "* Easy to use for users\n",
    "* Scalable (dynamically/automatically)\n",
    "* Responsive/Interactive\n",
    "* **Utilize currently deployed hardware/middleware**\n",
    "* **Minimally intrusive for site administrators**\n",
    "* In addition it is important to get work (‘effort’ & CPU) accounted for by CMS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Kubernetes (k8s)?\n",
    "\n",
    "Stack Overflow blog: \"*Kubernetes is about six years old, and over the last two years, it has risen in popularity to consistently be one of the most loved platforms. This year, it comes in as the number three most loved platform.*\"\n",
    "\n",
    "**Kubernetes is a platform that allows to run and orchestrate container workloads.**\n",
    "\n",
    "![k8s](https://d33wubrfki0l68.cloudfront.net/69e55f968a6f44613384615c6a78b881bfe28bd6/42cd3/_common-resources/images/flower.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why Kubernetes (k8s)?\n",
    "\n",
    "* <span style=\"color:orange\">Kubernetes is very extensible, and developers love that.</span>\n",
    "There are a set of existing resources like Pods, Deployments, StatefulSets, Secrets, ConfigMaps, etc. However, users and developers can add more resources in the form of Custom Resource Definitions.\n",
    "* <span style=\"color:green\"> Infrastructure as YAML. </span>\n",
    "All the resources in Kubernetes, can simply be expressed in a YAML file.\n",
    "* <span style=\"color:blue\">Scalability. </span>\n",
    "Software can be deployed for the first time in a scale-out manner across Pods, and deployments can be scaled in or out at any time.\n",
    "* <span style=\"color:red\">Time savings. </span>\n",
    "Pause a deployment at any time and resume it later.\n",
    "* <span style=\"color:purple\">Version control. </span>\n",
    "Update deployed Pods using newer versions of application images and roll back to an earlier deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why Kubernetes (k8s)?\n",
    " \n",
    "* **Horizontal autoscaling.** \n",
    "Kubernetes autoscalers automatically size a deployment’s number of Pods based on the usage of specified resources (within defined limits).\n",
    "\n",
    "* **Rolling updates.**\n",
    "Updates to a Kubernetes deployment are orchestrated in “rolling fashion,” across the deployment’s Pods. These rolling updates are orchestrated while working with optional predefined limits on the number of Pods that can be unavailable and the number of spare Pods that may exist temporarily.\n",
    "\n",
    "* **Canary deployments.**\n",
    "A useful pattern when deploying a new version of a deployment is to first test the new deployment in production, in parallel with the previous version, and scale up the new deployment while simultaneously scaling down the previous deployment.\n",
    "\n",
    "* **Security and Controls.**\n",
    "YAML is a great way to validate what and how things get deployed in Kubernetes. For example, one of the significant concerns when it comes to security is whether your workloads are running as a non-root user.\n",
    "\n",
    "*  <span style=\"color:purple\">**Another big aspect of Kubernetes popularity is its strong community.**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proposed Analysis Facility @ T2 Nebraska\n",
    "\n",
    "<img src=\"pics/coffea-casa.png\" width=\"900\" height=\"900\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proposed Analysis Facility @ T2 Nebraska: challenges\n",
    "\n",
    "<img src=\"pics/coffea-casa-challenge.png\" width=\"900\" height=\"900\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proposed Analysis Facility @ T2 Nebraska: configuration\n",
    "<img src=\"pics/coffea-casa1.png\" width=\"900\" height=\"900\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proposed Analysis Facility @ T2 Nebraska: configuration\n",
    "<img src=\"pics/coffea-casa2.png\" width=\"900\" height=\"900\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Proposed Analysis Facility @ T2 Nebraska: configuration\n",
    "<img src=\"pics/coffea-casa3.png\" width=\"700\" height=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coffea-casa: why you would love it?\n",
    "\n",
    "* <span style=\"color:blue\">No x509 </span>: authentification enabled via CILogin and *WLCG Bearear Tockens** (macaroons)\n",
    "* <span style=\"color:purple\">Security</span>: enabled TLS protocol over TCP sockets\n",
    "* <span style=\"color:green\"> No need to think about xrootd </span>: **We use XCache with new XRootD autorization plugin**\n",
    "* <span style=\"color:red\"> Access to \"grid-style\" analysis but from Python notebook!</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XCache as an access point to \"Anydata, Anytime, Anywhere\" (AAA)\n",
    "\n",
    "\n",
    "* *Xcache is a Squid-like cache*, but it primarily uses the “xroot” protocol, with HTTP protocol being added on. \n",
    "* *XCache provides a multi-threaded file caching application that can asynchronously fetch and cache file segments or whole files*.\n",
    "* Its primary design use case is caching static scientific data files of any format, large or small.\n",
    "* *Xcache is built upon Xrootd* and is flexible to be customized for many usage scenarios, via configuration or plugins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## XRootd authorization plugin\n",
    "\n",
    "* Code: https://github.com/bbockelm/xrdcl-authz-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for token in file /etc/cmsaf-secrets/xcache_token\n",
      "Looking for token in file /etc/cmsaf-secrets/xcache_token\n",
      "[3.65GB/3.65GB][100%][==================================================][109.9MB/s]   \n"
     ]
    }
   ],
   "source": [
    "! xrdcp -f root://xcache//store/data/Run2017B/SingleElectron/MINIAOD/31Mar2018-v1/60000/9E0F8458-EA37-E811-93F1-008CFAC919F0.root /dev/null  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coffea-casa secrets\n",
    "\n",
    "All secrets are available in the directory */etc/cmsaf-secrets* at container startup.\n",
    "\n",
    "* */etc/cmsaf-secrets/condor_token* is a condor IDTOKEN useful for submitting to T3.\n",
    "* */etc/cmsaf-secrets/ca.key* is a CA private key useful for Dask\n",
    "* */etc/cmsaf-secrets/ca.pem* is a CA public key useful for Dask\n",
    "* */etc/cmsaf-secrets/hostcert.pem* is a host certificate and private key useful for the Dask scheduler.\n",
    "* */etc/cmsaf-secrets/usercert.pem* is a user certificate and private key useful for the Dask workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Coffea-casa building blocks\n",
    "\n",
    "* JupyterHub setup (Helm K8s charts): https://github.com/CoffeaTeam/jhub (except of specific secrets)\n",
    "* Docker images for Dask Scheduler and Worker: https://github.com/CoffeaTeam/coffea-casa\n",
    "  * https://hub.docker.com/r/coffeateam/coffea-casa\n",
    "  * https://hub.docker.com/r/coffeateam/coffea-casa-analysis\n",
    "* Docker image for JupyterHub (to get authentification macaroons in the launch environment)\n",
    "https://github.com/clundst/jhubDocker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When you will be able to use it?\n",
    "\n",
    "* Alfa version is expected to be available soon for preliminary tests by CMS community (ask your friend for an invitation :D).\n",
    "* Fully available during fall/winter 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demo time\n",
    "\n",
    "... lets try to see how it works!\n",
    "\n",
    "\n",
    "  1. [Simple loop over events](adl1_tls.ipynb)\n",
    "  2. [More complex benchmark](adl8.ipynb)\n",
    "  3. [Using XCache](coffea_xcache.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
